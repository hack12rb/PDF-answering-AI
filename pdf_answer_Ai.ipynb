{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd57aed5-2310-4833-b9a7-f0842dcabcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.24.5)\n",
      "Requirement already satisfied: gensim in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: flask in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pymupdf) (1.24.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from gensim) (2.0.9)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from flask) (2.2.3)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from flask) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\hp\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from Jinja2>=3.0->flask) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n",
      "Requirement already satisfied: simpful==2.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
      "Requirement already satisfied: fst-pso==1.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf gensim nltk flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ddc1b0d-ce58-410e-8afb-1453ecff436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = r\"C:\\Users\\HP\\OneDrive\\Desktop\\vlg image\\vlg_image_denoising_report.pdf\"\n",
    "pdf_text = extract_text_from_pdf(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5052ad56-ae68-46f5-9e1f-8b2d77bc297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e0fb71-e5e9-422f-bf92-d00cd19bc739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure you have downloaded the necessary NLTK data files\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03e9e891-d14b-4736-9454-69910035f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "preprocessed_text = preprocess_text(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a231bc57-22dd-4ef1-8ce7-6f81ab99e31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d08d0cba-9f14-4298-bbb2-6e7bcba9ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train Word2Vec model (or load a pre-trained model)\n",
    "model = Word2Vec([preprocessed_text], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b627dd0-d242-46e1-8a1f-d403fc609359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c213ef3-43b5-4de1-8fca-1eb2561cb7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model):\n",
    "    words = preprocess_text(text)\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d832102b-3bad-4885-898d-bcc38ee86051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_answer(question, context, model):\n",
    "    question_embedding = get_embedding(question, model)\n",
    "    context_sentences = context.split('.')\n",
    "    best_similarity = -1\n",
    "    best_sentence = None\n",
    "    for sentence in context_sentences:\n",
    "        sentence_embedding = get_embedding(sentence, model)\n",
    "        similarity = cosine_similarity([question_embedding], [sentence_embedding])[0][0]\n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_sentence = sentence\n",
    "    return best_sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1522c1b-8c4f-4775-9bef-35836e583fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      "Model Layers and Parameters\n",
      "‚óè\n",
      "The Model has been made using 5 convolutional layers, 2 max pooling layers and\n",
      "2 Sampling layers, these are key parts of the Model Architecture which is used to\n",
      "remove noise from low quality noisy images and convert it to better quality\n",
      "images which are closer to the clean images without noise\n"
     ]
    }
   ],
   "source": [
    "question = \"model parameters?\"\n",
    "answer = find_best_answer(question, pdf_text, model)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cba862-c0c1-4b2e-994b-5a941cf79f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
